{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from pc2 import Network, Module, Dense, Sequential\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from clu import metrics\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import datasets\n",
    "import numpy.random as npr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc1301249d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "plt.imshow(jnp.reshape(train_images[0], (28,28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, params=None):\n",
    "        if params is None:\n",
    "            x1 = nn.Dense(1024)(x)\n",
    "        else:\n",
    "            x1 = nn.Dense(\n",
    "                features=1024,\n",
    "                kernel_init=lambda r,s,d:params['Dense_0']['kernel'],\n",
    "                bias_init=lambda r,s,d:params['Dense_0']['bias'])(x)\n",
    "        x_1 = self.variable('x_i', 'x_1', lambda s:x1, x1.shape).value\n",
    "        x2 = nn.tanh(x_1)\n",
    "        if params is None:\n",
    "            x2 = nn.Dense(256)(x2)\n",
    "        else:\n",
    "            x2 = nn.Dense(\n",
    "                features=256,\n",
    "                kernel_init=lambda r,s,d:params['Dense_1']['kernel'],\n",
    "                bias_init=lambda r,s,d:params['Dense_1']['bias'])(x2)\n",
    "        x_2 = self.variable('x_i', 'x_2', lambda s:x2, x2.shape).value\n",
    "        x3 = nn.tanh(x_2)\n",
    "        if params is None:\n",
    "            y = nn.Dense(10)(x3)\n",
    "        else:\n",
    "            y = nn.Dense(\n",
    "                features=10,\n",
    "                kernel_init=lambda r,s,d:params['Dense_2']['kernel'],\n",
    "                bias_init=lambda r,s,d:params['Dense_2']['bias'])(x3)\n",
    "        # y = nn.sigmoid(y)\n",
    "        y = nn.activation.softmax(y)\n",
    "        # y = nn.activation.softmax(nn.Dense(10)(x_2))\n",
    "        energy_i = 0.5 * jnp.sum((x_1 - x1) ** 2, axis=-1) + 0.5 * jnp.sum((x_2 - x2) ** 2, axis=-1)\n",
    "        return y, energy_i\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     theta_0 = self.get_variable('params', 'Dense_0')\n",
    "    #     x1 = nn.tanh(jnp.einsum('io,bi->bo', theta_0['kernel'], x) + theta_0['bias'])\n",
    "    #     self.put_variable('x_i', 'x_1', x1)\n",
    "    #     theta_y = self.get_variable('params', 'Dense_1')\n",
    "    #     y = jnp.einsum('io,bi->bo', theta_y['kernel'], x1) + theta_y['bias']\n",
    "    #     y = nn.activation.softmax(y)\n",
    "    #     return y\n",
    "\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 784) (123, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': (1024,), 'kernel': (784, 1024)},\n",
       "  'Dense_1': {'bias': (256,), 'kernel': (1024, 256)},\n",
       "  'Dense_2': {'bias': (10,), 'kernel': (256, 10)}},\n",
       " 'x_i': {'x_1': (123, 1024), 'x_2': (123, 256)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "train_size=123\n",
    "x = train_images[0:train_size]\n",
    "y = train_labels[0:train_size]\n",
    "print(x.shape, y.shape)\n",
    "variables = model.init(key2, x) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, variables) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.10963639, 0.08086108, 0.0792252 , ..., 0.0905846 , 0.08625005,\n",
       "         0.13718134],\n",
       "        [0.08564303, 0.0535102 , 0.1259291 , ..., 0.0852263 , 0.1024547 ,\n",
       "         0.13966812],\n",
       "        [0.08987457, 0.12355465, 0.09319659, ..., 0.1125367 , 0.0963291 ,\n",
       "         0.11611517],\n",
       "        ...,\n",
       "        [0.09316536, 0.06430121, 0.10110824, ..., 0.10009388, 0.06868123,\n",
       "         0.19100423],\n",
       "        [0.09473741, 0.05864696, 0.10425573, ..., 0.08206704, 0.11411878,\n",
       "         0.131278  ],\n",
       "        [0.0932291 , 0.10230389, 0.13432024, ..., 0.05894828, 0.10857943,\n",
       "         0.13367994]], dtype=float32),\n",
       " Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(variables, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "# @jax.jit\n",
    "def energy_fn_(theta, x_i, x_batched, y_batched, alpha=1.0):\n",
    "  def cross_entropy(x, y):\n",
    "    pred, energy_i = model.apply({'params': theta, 'x_i': x_i}, x)\n",
    "    cse = -jnp.sum(y * jnp.log(pred), axis=-1)\n",
    "    result = cse + alpha * energy_i\n",
    "    # result = 0.5 * jnp.sum((pred - y) ** 2, axis=-1) + alpha * energy_i\n",
    "    # print(f'result: {result.shape}, {jnp.mean(result, axis=0)}')\n",
    "    return result, cse, energy_i\n",
    "  # energy = jnp.mean(jax.vmap(cross_entropy)(x_batched,y_batched), axis=0)\n",
    "  energy, cse, energy_i = cross_entropy(x_batched,y_batched)\n",
    "  cse = jnp.mean(cse, axis=0)\n",
    "  # print(f'cse: {cse.shape} {cse}')\n",
    "  energy_i = jnp.mean(energy_i, axis=0)\n",
    "  # print(f'energy_i: {energy_i.shape} {energy_i}')\n",
    "  energy = jnp.mean(energy, axis=0)\n",
    "  # print(f'energy: {energy.shape}, {energy}')\n",
    "  return energy, cse, energy_i\n",
    "\n",
    "# @jax.jit\n",
    "def energy_fn(theta, x_i, x_batched, y_batched, alpha=1.0):\n",
    "  return energy_fn_(theta, x_i, x_batched, y_batched, alpha)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(variables, batch):\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(model.apply(variables, inputs)[0], axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function energy_fn at 0x7fbffd08f060> <function energy_fn at 0x7fbfa84182c0>\n",
      "energy: 2.335115909576416\n",
      "cse: 2.335115909576416\n",
      "energy_i: 0.0\n"
     ]
    }
   ],
   "source": [
    "T = 100 # max IL iterations\n",
    "eplison = 0.0001 # IL eplison\n",
    "x_i_lr = 0.5  # Gradient step size for IL\n",
    "theta_lr = 0.1 # Gradient step size for Theta\n",
    "theta_grad_fn = jax.value_and_grad(energy_fn, argnums=0)\n",
    "x_i_grad_fn = jax.value_and_grad(energy_fn, argnums=1)\n",
    "print(theta_grad_fn, x_i_grad_fn)\n",
    "\n",
    "@jax.jit\n",
    "def update_theta(theta, learning_rate, grads):\n",
    "  theta = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, theta, grads)\n",
    "  return theta\n",
    "\n",
    "@jax.jit\n",
    "def update_x_i(x_i, learning_rate, grads):\n",
    "  x_i = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, x_i, grads)\n",
    "  return x_i\n",
    "\n",
    "# x, y = next(batches)\n",
    "# variables = model.init(key2, x)\n",
    "\n",
    "# variables = model.init(key2, train_images, params=theta)\n",
    "energy, cse, energy_i = energy_fn_(variables['params'], variables['x_i'], x, y)\n",
    "print(f'energy: {energy}')\n",
    "print(f'cse: {cse}')\n",
    "print(f'energy_i: {energy_i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IL energy: [2.316e+00 => 2.315e+00] (0)\n",
      "IL energy: [2.285e+00 => 2.284e+00] (0)\n",
      "IL energy: [2.253e+00 => 2.253e+00] (0)\n",
      "IL energy: [2.232e+00 => 2.231e+00] (0)\n",
      "IL energy: [2.211e+00 => 2.211e+00] (0)\n",
      "IL energy: [2.189e+00 => 2.189e+00] (0)\n",
      "IL energy: [2.167e+00 => 2.167e+00] (0)\n",
      "IL energy: [2.140e+00 => 2.140e+00] (0)\n",
      "IL energy: [2.117e+00 => 2.117e+00] (0)\n",
      "IL energy: [2.099e+00 => 2.099e+00] (0)\n",
      "IL energy: [2.088e+00 => 2.088e+00] (0)\n",
      "IL energy: [2.064e+00 => 2.064e+00] (0)\n",
      "IL energy: [2.043e+00 => 2.043e+00] (0)\n",
      "IL energy: [2.021e+00 => 2.020e+00] (0)\n",
      "IL energy: [1.995e+00 => 1.968e+00] [end] (99)\n",
      "Loss step 0\n",
      "Training set accuracy 49.15%\n",
      "Test set accuracy 48.95%\n",
      "IL energy: [1.977e+00 => 1.977e+00] (0)\n",
      "IL energy: [1.964e+00 => 1.964e+00] (0)\n",
      "IL energy: [1.946e+00 => 1.946e+00] (0)\n",
      "IL energy: [1.935e+00 => 1.935e+00] (0)\n",
      "IL energy: [1.913e+00 => 1.912e+00] (0)\n",
      "IL energy: [1.885e+00 => 1.884e+00] (0)\n",
      "IL energy: [1.885e+00 => 1.885e+00] (0)\n",
      "IL energy: [1.868e+00 => 1.868e+00] (0)\n",
      "IL energy: [1.854e+00 => 1.853e+00] (3)\n",
      "IL energy: [1.839e+00 => 1.836e+00] (13)\n",
      "IL energy: [1.825e+00 => 1.818e+00] (34)\n",
      "IL energy: [1.807e+00 => 1.795e+00] (60)\n",
      "IL energy: [1.783e+00 => 1.768e+00] (82)\n",
      "IL energy: [1.769e+00 => 1.751e+00] [end] (99)\n",
      "IL energy: [1.754e+00 => 1.727e+00] [end] (99)\n",
      "Loss step 1\n",
      "Training set accuracy 63.77%\n",
      "Test set accuracy 64.28%\n",
      "IL energy: [1.735e+00 => 1.717e+00] [end] (99)\n",
      "IL energy: [1.726e+00 => 1.708e+00] [end] (99)\n",
      "IL energy: [1.705e+00 => 1.687e+00] [end] (99)\n",
      "IL energy: [1.695e+00 => 1.677e+00] [end] (99)\n",
      "IL energy: [1.680e+00 => 1.662e+00] [end] (99)\n",
      "IL energy: [1.661e+00 => 1.643e+00] [end] (99)\n",
      "IL energy: [1.655e+00 => 1.637e+00] [end] (99)\n",
      "IL energy: [1.642e+00 => 1.624e+00] [end] (99)\n",
      "IL energy: [1.623e+00 => 1.605e+00] [end] (99)\n",
      "IL energy: [1.601e+00 => 1.583e+00] [end] (99)\n",
      "IL energy: [1.596e+00 => 1.578e+00] [end] (99)\n",
      "IL energy: [1.587e+00 => 1.569e+00] [end] (99)\n",
      "IL energy: [1.551e+00 => 1.533e+00] [end] (99)\n",
      "IL energy: [1.565e+00 => 1.546e+00] [end] (99)\n",
      "IL energy: [1.553e+00 => 1.525e+00] [end] (99)\n",
      "Loss step 2\n",
      "Training set accuracy 70.75%\n",
      "Test set accuracy 71.58%\n",
      "IL energy: [1.513e+00 => 1.495e+00] [end] (99)\n",
      "IL energy: [1.513e+00 => 1.494e+00] [end] (99)\n",
      "IL energy: [1.506e+00 => 1.488e+00] [end] (99)\n",
      "IL energy: [1.496e+00 => 1.478e+00] [end] (99)\n",
      "IL energy: [1.475e+00 => 1.456e+00] [end] (99)\n",
      "IL energy: [1.478e+00 => 1.459e+00] [end] (99)\n",
      "IL energy: [1.466e+00 => 1.448e+00] [end] (99)\n",
      "IL energy: [1.453e+00 => 1.435e+00] [end] (99)\n",
      "IL energy: [1.441e+00 => 1.423e+00] [end] (99)\n",
      "IL energy: [1.435e+00 => 1.416e+00] [end] (99)\n",
      "IL energy: [1.415e+00 => 1.396e+00] [end] (99)\n",
      "IL energy: [1.407e+00 => 1.388e+00] [end] (99)\n",
      "IL energy: [1.411e+00 => 1.392e+00] [end] (99)\n",
      "IL energy: [1.390e+00 => 1.372e+00] [end] (99)\n",
      "IL energy: [1.372e+00 => 1.344e+00] [end] (99)\n",
      "Loss step 3\n",
      "Training set accuracy 74.38%\n",
      "Test set accuracy 75.08%\n",
      "IL energy: [1.364e+00 => 1.346e+00] [end] (99)\n",
      "IL energy: [1.347e+00 => 1.328e+00] [end] (99)\n",
      "IL energy: [1.348e+00 => 1.330e+00] [end] (99)\n",
      "IL energy: [1.337e+00 => 1.319e+00] [end] (99)\n",
      "IL energy: [1.330e+00 => 1.312e+00] [end] (99)\n",
      "IL energy: [1.329e+00 => 1.311e+00] [end] (99)\n",
      "IL energy: [1.313e+00 => 1.295e+00] [end] (99)\n",
      "IL energy: [1.310e+00 => 1.291e+00] [end] (99)\n",
      "IL energy: [1.282e+00 => 1.263e+00] [end] (99)\n",
      "IL energy: [1.293e+00 => 1.274e+00] [end] (99)\n",
      "IL energy: [1.280e+00 => 1.261e+00] [end] (99)\n",
      "IL energy: [1.286e+00 => 1.267e+00] [end] (99)\n",
      "IL energy: [1.263e+00 => 1.244e+00] [end] (99)\n",
      "IL energy: [1.239e+00 => 1.221e+00] [end] (99)\n",
      "IL energy: [1.239e+00 => 1.211e+00] [end] (99)\n",
      "Loss step 4\n",
      "Training set accuracy 76.75%\n",
      "Test set accuracy 77.61%\n",
      "IL energy: [1.238e+00 => 1.220e+00] [end] (99)\n",
      "IL energy: [1.237e+00 => 1.218e+00] [end] (99)\n",
      "IL energy: [1.218e+00 => 1.199e+00] [end] (99)\n",
      "IL energy: [1.217e+00 => 1.198e+00] [end] (99)\n",
      "IL energy: [1.212e+00 => 1.194e+00] [end] (99)\n",
      "IL energy: [1.195e+00 => 1.176e+00] [end] (99)\n",
      "IL energy: [1.196e+00 => 1.177e+00] [end] (99)\n",
      "IL energy: [1.180e+00 => 1.161e+00] [end] (99)\n",
      "IL energy: [1.183e+00 => 1.164e+00] [end] (99)\n",
      "IL energy: [1.177e+00 => 1.158e+00] [end] (99)\n",
      "IL energy: [1.162e+00 => 1.143e+00] [end] (99)\n",
      "IL energy: [1.159e+00 => 1.140e+00] [end] (99)\n",
      "IL energy: [1.154e+00 => 1.135e+00] [end] (99)\n",
      "IL energy: [1.134e+00 => 1.115e+00] [end] (99)\n",
      "IL energy: [1.140e+00 => 1.111e+00] [end] (99)\n",
      "Loss step 5\n",
      "Training set accuracy 78.29%\n",
      "Test set accuracy 79.37%\n",
      "IL energy: [1.149e+00 => 1.130e+00] [end] (99)\n",
      "IL energy: [1.121e+00 => 1.102e+00] [end] (99)\n",
      "IL energy: [1.112e+00 => 1.094e+00] [end] (99)\n",
      "IL energy: [1.100e+00 => 1.081e+00] [end] (99)\n",
      "IL energy: [1.103e+00 => 1.084e+00] [end] (99)\n",
      "IL energy: [1.116e+00 => 1.097e+00] [end] (99)\n",
      "IL energy: [1.096e+00 => 1.077e+00] [end] (99)\n",
      "IL energy: [1.088e+00 => 1.069e+00] [end] (99)\n",
      "IL energy: [1.081e+00 => 1.062e+00] [end] (99)\n",
      "IL energy: [1.071e+00 => 1.052e+00] [end] (99)\n",
      "IL energy: [1.090e+00 => 1.071e+00] [end] (99)\n",
      "IL energy: [1.070e+00 => 1.051e+00] [end] (99)\n",
      "IL energy: [1.056e+00 => 1.037e+00] [end] (99)\n",
      "IL energy: [1.076e+00 => 1.057e+00] [end] (99)\n",
      "IL energy: [1.058e+00 => 1.028e+00] [end] (99)\n",
      "Loss step 6\n",
      "Training set accuracy 79.43%\n",
      "Test set accuracy 80.26%\n",
      "IL energy: [1.054e+00 => 1.034e+00] [end] (99)\n",
      "IL energy: [1.050e+00 => 1.031e+00] [end] (99)\n"
     ]
    }
   ],
   "source": [
    "theta = variables['params']\n",
    "\n",
    "for step in range(501):\n",
    "  # Perform one gradient update.\n",
    "  for _ in range(num_batches):\n",
    "    x, y = next(batches)\n",
    "    variables = model.init(key2, x, params=theta)\n",
    "    theta = variables['params']\n",
    "    x_i = variables['x_i']\n",
    "    start_energy = energy_fn(theta, x_i, x, y)\n",
    "    prev_energy = start_energy\n",
    "    for t, _ in enumerate(range(T)):\n",
    "      _, grads = x_i_grad_fn(theta, x_i, x, y)\n",
    "      x_i = update_x_i(x_i, x_i_lr, grads)\n",
    "      new_energy = energy_fn(theta, x_i, x, y)\n",
    "      delta = prev_energy - new_energy\n",
    "      prev_energy = new_energy\n",
    "      if delta/new_energy < eplison:\n",
    "        print(f'IL energy: [{start_energy:.3e} => {new_energy:.3e}] ({t})')\n",
    "        break\n",
    "      if t == T-1:\n",
    "        print(f'IL energy: [{start_energy:.3e} => {new_energy:.3e}] [end] ({t})')\n",
    "\n",
    "    _, grads = theta_grad_fn(theta, x_i, x, y)\n",
    "    theta = update_theta(theta, theta_lr, grads)\n",
    "\n",
    "\n",
    "  if step % 1 == 0:\n",
    "    print(f'Loss step {step}')\n",
    "    variables = model.init(key2, train_images, params=theta)\n",
    "    train_acc = accuracy(variables, (train_images, train_labels))\n",
    "    variables = model.init(key2, test_images, params=theta)\n",
    "    test_acc = accuracy(variables, (test_images, test_labels))\n",
    "    print(f\"Training set accuracy {train_acc*100:0.2f}%\")\n",
    "    print(f\"Test set accuracy {test_acc*100:0.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[6.8150144e+00, 1.1460138e-03, 1.1801200e+00],\n",
       "       [2.1912851e+00, 2.3726006e+00, 1.1289402e+00],\n",
       "       [2.9349127e-01, 2.8977572e-04, 7.4130140e-02]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jax.random.normal(key1, (3,3))\n",
    "b = jax.random.normal(key1, (3,3))\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_1': Array([[-0.13462605, -0.3278432 ,  0.22782673, ..., -0.10996187,\n",
      "        -0.49576125,  0.2525089 ],\n",
      "       [ 0.02520173, -0.22533576,  0.7370963 , ..., -0.4481546 ,\n",
      "        -0.02872543,  0.38858265],\n",
      "       [ 0.10196313,  0.07504214, -0.06939081, ...,  0.11857884,\n",
      "        -0.21023458,  0.27247092],\n",
      "       ...,\n",
      "       [ 0.00991802,  0.08777118, -0.07607331, ..., -0.03389303,\n",
      "        -0.42004925,  0.45156288],\n",
      "       [ 0.31259665, -0.01424194,  0.6128926 , ..., -0.3326202 ,\n",
      "        -0.1322913 , -0.15132794],\n",
      "       [ 0.32569477, -0.5153029 ,  0.20282543, ..., -0.22639666,\n",
      "        -0.20922866,  0.07548092]], dtype=float32), 'x_2': Array([[-0.39323556,  1.7077905 , -0.33184367, ...,  0.21103896,\n",
      "        -0.05986749,  0.72051615],\n",
      "       [ 0.1705095 ,  2.6917632 , -0.01869105, ...,  0.21125521,\n",
      "        -0.04514058,  1.1818498 ],\n",
      "       [-0.18432917,  0.8543403 ,  0.21080086, ..., -0.02238091,\n",
      "        -0.43819115,  0.21665443],\n",
      "       ...,\n",
      "       [ 0.06171044,  2.8494017 ,  0.1912569 , ...,  0.3959846 ,\n",
      "        -0.15330265,  0.95327973],\n",
      "       [ 0.24003059,  2.577035  ,  0.04914803, ...,  0.01286555,\n",
      "         0.06704608,  1.2378156 ],\n",
      "       [ 0.1969547 ,  3.8650684 ,  0.3820875 , ..., -0.05010581,\n",
      "         0.06936268,  0.60998505]], dtype=float32)}\n",
      "(10000, 1024)\n",
      "(10000, 256)\n"
     ]
    }
   ],
   "source": [
    "print(variables['x_i'])\n",
    "print(variables['x_i']['x_1'].shape)\n",
    "print(variables['x_i']['x_2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
