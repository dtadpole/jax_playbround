{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from pc2 import Network, Module, Dense, Sequential\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from clu import metrics\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import datasets\n",
    "import numpy.random as npr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4e70259f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "plt.imshow(jnp.reshape(train_images[0], (28,28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # def setup(self):\n",
    "    #     self.dense1 = nn.Dense(1024)\n",
    "    #     self.dense2 = nn.Dense(10)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(1024)(x)\n",
    "        # x = self.dense1(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(10)(x)\n",
    "        # x = self.dense2(x)\n",
    "        x = nn.activation.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784) (1000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': (1024,), 'kernel': (784, 1024)},\n",
       "  'Dense_1': {'bias': (10,), 'kernel': (1024, 10)}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "train_size=1000\n",
    "x = train_images[0:train_size]\n",
    "y = train_labels[0:train_size]\n",
    "print(x.shape, y.shape)\n",
    "params = model.init(key2, x) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.04699855, 0.07517353, 0.07229667, ..., 0.13136439, 0.1209725 ,\n",
       "        0.06899231],\n",
       "       [0.04957927, 0.07826361, 0.0543289 , ..., 0.09494812, 0.114541  ,\n",
       "        0.07375124],\n",
       "       [0.07304829, 0.07933359, 0.10769977, ..., 0.13269468, 0.10561708,\n",
       "        0.07004561],\n",
       "       ...,\n",
       "       [0.05949913, 0.08842745, 0.06820785, ..., 0.07883118, 0.09160868,\n",
       "        0.10491852],\n",
       "       [0.08793209, 0.09765116, 0.06932477, ..., 0.09212617, 0.10404621,\n",
       "        0.08604551],\n",
       "       [0.08983723, 0.11864541, 0.06745436, ..., 0.14635032, 0.09474164,\n",
       "        0.08494898]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def cross_entropy_loss(params, x_batched, y_batched):\n",
    "  def cross_entropy(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return -jnp.sum(y * jnp.log(pred))\n",
    "  return jnp.mean(jax.vmap(cross_entropy)(x_batched,y_batched), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(model.apply(params, inputs), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function cross_entropy_loss at 0x7f4e70181c60>\n",
      "Loss step 0:  0.53155446\n",
      "Training set accuracy 86.86%\n",
      "Test set accuracy 87.72%\n",
      "Loss step 10:  0.2999451\n",
      "Training set accuracy 91.64%\n",
      "Test set accuracy 91.85%\n",
      "Loss step 20:  0.2615211\n",
      "Training set accuracy 92.41%\n",
      "Test set accuracy 92.60%\n",
      "Loss step 30:  0.2953677\n",
      "Training set accuracy 93.10%\n",
      "Test set accuracy 93.05%\n",
      "Loss step 40:  0.17308256\n",
      "Training set accuracy 93.86%\n",
      "Test set accuracy 93.62%\n",
      "Loss step 50:  0.22357145\n",
      "Training set accuracy 94.53%\n",
      "Test set accuracy 94.21%\n",
      "Loss step 60:  0.17435804\n",
      "Training set accuracy 95.05%\n",
      "Test set accuracy 94.63%\n",
      "Loss step 70:  0.17509212\n",
      "Training set accuracy 95.71%\n",
      "Test set accuracy 95.24%\n",
      "Loss step 80:  0.14026102\n",
      "Training set accuracy 96.08%\n",
      "Test set accuracy 95.71%\n",
      "Loss step 90:  0.103190586\n",
      "Training set accuracy 96.53%\n",
      "Test set accuracy 96.04%\n",
      "Loss step 100:  0.12776417\n",
      "Training set accuracy 96.89%\n",
      "Test set accuracy 96.33%\n",
      "Loss step 110:  0.11034234\n",
      "Training set accuracy 97.15%\n",
      "Test set accuracy 96.51%\n",
      "Loss step 120:  0.08295543\n",
      "Training set accuracy 97.39%\n",
      "Test set accuracy 96.62%\n",
      "Loss step 130:  0.0846257\n",
      "Training set accuracy 97.62%\n",
      "Test set accuracy 96.77%\n",
      "Loss step 140:  0.067090265\n",
      "Training set accuracy 97.82%\n",
      "Test set accuracy 96.93%\n",
      "Loss step 150:  0.08567417\n",
      "Training set accuracy 97.94%\n",
      "Test set accuracy 97.11%\n",
      "Loss step 160:  0.09780285\n",
      "Training set accuracy 98.12%\n",
      "Test set accuracy 97.20%\n",
      "Loss step 170:  0.07531042\n",
      "Training set accuracy 98.24%\n",
      "Test set accuracy 97.27%\n",
      "Loss step 180:  0.041827504\n",
      "Training set accuracy 98.39%\n",
      "Test set accuracy 97.29%\n",
      "Loss step 190:  0.060928572\n",
      "Training set accuracy 98.46%\n",
      "Test set accuracy 97.32%\n",
      "Loss step 200:  0.04360339\n",
      "Training set accuracy 98.59%\n",
      "Test set accuracy 97.45%\n",
      "Loss step 210:  0.05198406\n",
      "Training set accuracy 98.70%\n",
      "Test set accuracy 97.52%\n",
      "Loss step 220:  0.049899224\n",
      "Training set accuracy 98.81%\n",
      "Test set accuracy 97.56%\n",
      "Loss step 230:  0.04342743\n",
      "Training set accuracy 98.88%\n",
      "Test set accuracy 97.59%\n",
      "Loss step 240:  0.051208716\n",
      "Training set accuracy 98.94%\n",
      "Test set accuracy 97.63%\n",
      "Loss step 250:  0.04022224\n",
      "Training set accuracy 99.01%\n",
      "Test set accuracy 97.68%\n",
      "Loss step 260:  0.030229913\n",
      "Training set accuracy 99.06%\n",
      "Test set accuracy 97.73%\n",
      "Loss step 270:  0.04240123\n",
      "Training set accuracy 99.13%\n",
      "Test set accuracy 97.77%\n",
      "Loss step 280:  0.04346806\n",
      "Training set accuracy 99.20%\n",
      "Test set accuracy 97.79%\n",
      "Loss step 290:  0.032105297\n",
      "Training set accuracy 99.24%\n",
      "Test set accuracy 97.82%\n",
      "Loss step 300:  0.023487967\n",
      "Training set accuracy 99.33%\n",
      "Test set accuracy 97.86%\n",
      "Loss step 310:  0.019056857\n",
      "Training set accuracy 99.36%\n",
      "Test set accuracy 97.91%\n",
      "Loss step 320:  0.032427866\n",
      "Training set accuracy 99.41%\n",
      "Test set accuracy 97.88%\n",
      "Loss step 330:  0.02949337\n",
      "Training set accuracy 99.49%\n",
      "Test set accuracy 97.92%\n",
      "Loss step 340:  0.04239329\n",
      "Training set accuracy 99.50%\n",
      "Test set accuracy 97.87%\n",
      "Loss step 350:  0.026723947\n",
      "Training set accuracy 99.53%\n",
      "Test set accuracy 97.92%\n",
      "Loss step 360:  0.018318644\n",
      "Training set accuracy 99.57%\n",
      "Test set accuracy 97.90%\n",
      "Loss step 370:  0.018061966\n",
      "Training set accuracy 99.60%\n",
      "Test set accuracy 97.92%\n",
      "Loss step 380:  0.020497577\n",
      "Training set accuracy 99.63%\n",
      "Test set accuracy 97.91%\n",
      "Loss step 390:  0.024423596\n",
      "Training set accuracy 99.65%\n",
      "Test set accuracy 97.95%\n",
      "Loss step 400:  0.023634488\n",
      "Training set accuracy 99.69%\n",
      "Test set accuracy 98.00%\n",
      "Loss step 410:  0.038482796\n",
      "Training set accuracy 99.71%\n",
      "Test set accuracy 98.00%\n",
      "Loss step 420:  0.018228173\n",
      "Training set accuracy 99.73%\n",
      "Test set accuracy 97.99%\n",
      "Loss step 430:  0.015010031\n",
      "Training set accuracy 99.76%\n",
      "Test set accuracy 98.00%\n",
      "Loss step 440:  0.01638102\n",
      "Training set accuracy 99.78%\n",
      "Test set accuracy 98.02%\n",
      "Loss step 450:  0.010420773\n",
      "Training set accuracy 99.78%\n",
      "Test set accuracy 98.02%\n",
      "Loss step 460:  0.016064245\n",
      "Training set accuracy 99.81%\n",
      "Test set accuracy 98.01%\n",
      "Loss step 470:  0.015717193\n",
      "Training set accuracy 99.82%\n",
      "Test set accuracy 98.02%\n",
      "Loss step 480:  0.0153779825\n",
      "Training set accuracy 99.84%\n",
      "Test set accuracy 97.97%\n",
      "Loss step 490:  0.009745492\n",
      "Training set accuracy 99.85%\n",
      "Test set accuracy 98.01%\n",
      "Loss step 500:  0.01262606\n",
      "Training set accuracy 99.87%\n",
      "Test set accuracy 98.04%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1  # Gradient step size.\n",
    "loss_grad_fn = jax.value_and_grad(cross_entropy_loss)\n",
    "print(loss_grad_fn)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "for i in range(501):\n",
    "  # Perform one gradient update.\n",
    "  for _ in range(num_batches):\n",
    "    x, y = next(batches)\n",
    "    loss_val, grads = loss_grad_fn(params, x, y)\n",
    "    params = update_params(params, learning_rate, grads)\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(f'Loss step {i}: ', loss_val)\n",
    "    train_acc = accuracy(params, (train_images, train_labels))\n",
    "    test_acc = accuracy(params, (test_images, test_labels))\n",
    "    print(f\"Training set accuracy {train_acc*100:0.2f}%\")\n",
    "    print(f\"Test set accuracy {test_acc*100:0.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[6.8150144e+00, 1.1460138e-03, 1.1801200e+00],\n",
       "       [2.1912851e+00, 2.3726006e+00, 1.1289402e+00],\n",
       "       [2.9349127e-01, 2.8977572e-04, 7.4130140e-02]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jax.random.normal(key1, (3,3))\n",
    "b = jax.random.normal(key1, (3,3))\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Dense_0': {'bias': Array([-0.04238342,  0.0446732 ,  0.04759712, ..., -0.03485369,\n",
      "        0.03680721, -0.00253925], dtype=float32), 'kernel': Array([[ 0.07971383,  0.00994104, -0.03574973, ...,  0.00628182,\n",
      "         0.02120237,  0.02002901],\n",
      "       [ 0.04412211,  0.0430598 , -0.04036054, ...,  0.03124418,\n",
      "        -0.04736134, -0.02334021],\n",
      "       [ 0.01488651,  0.03339985,  0.03590554, ...,  0.04288763,\n",
      "        -0.01492494,  0.04700463],\n",
      "       ...,\n",
      "       [-0.00046571,  0.04048149, -0.03295735, ...,  0.0106256 ,\n",
      "         0.05399278,  0.0324134 ],\n",
      "       [-0.01125758, -0.00655999, -0.02332083, ...,  0.00946312,\n",
      "         0.00079829,  0.01359766],\n",
      "       [ 0.02846446, -0.0003765 ,  0.0099156 , ...,  0.00535347,\n",
      "         0.02542572, -0.07208928]], dtype=float32)}, 'Dense_1': {'bias': Array([-0.14579825,  0.31478858,  0.09580804, -0.12262518, -0.09328156,\n",
      "        0.45574516, -0.06302801,  0.32906225, -0.7084837 , -0.06215745],      dtype=float32), 'kernel': Array([[ 0.03785843, -0.06708796,  0.07577455, ..., -0.06327466,\n",
      "         0.13767764,  0.04021339],\n",
      "       [ 0.08310588,  0.1486273 , -0.06597716, ...,  0.06339181,\n",
      "         0.01983915, -0.19939287],\n",
      "       [-0.0137994 ,  0.1776505 ,  0.3708537 , ..., -0.101533  ,\n",
      "         0.09875904,  0.33713964],\n",
      "       ...,\n",
      "       [ 0.04247447,  0.12295645,  0.07136578, ..., -0.18582512,\n",
      "        -0.04790301,  0.11400912],\n",
      "       [-0.17618491,  0.10383126,  0.27689856, ...,  0.05809806,\n",
      "        -0.16218263,  0.22979023],\n",
      "       [-0.07857564, -0.01483271, -0.0392605 , ..., -0.01208381,\n",
      "         0.01960143, -0.06866007]], dtype=float32)}}}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
